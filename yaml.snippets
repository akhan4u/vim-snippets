snippet pod "Sample Nginx Pod manifest with Init Container" !b
---
apiVersion: v1
kind: Pod
metadata:
  name: ${1:nginx}
  namespace: ${2:default}
  labels:
    app: $1
spec:
  containers:
    - name: $1
      image: ${3:nginx:latest}
      ports:
        - containerPort: 80
      volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
      resources:
        limits:
          memory: '1Gi'
          cpu: '800m'
        requests:
          memory: '700Mi'
          cpu: '400m'
  # These containers are run during pod initialization
  initContainers:
    - name: install
      image: busybox
      command:
        - wget
        - '-O'
        - '/work-dir/index.html'
        - http://kubernetes.io
      volumeMounts:
        - name: workdir
          mountPath: '/work-dir'
  dnsPolicy: Default
  volumes:
    - name: workdir
      emptyDir: {}
endsnippet

snippet pod "Custom DNS configurations on the pod spec" !b
# In the Kubernetes cluster, by default, when running a pod with default DNS policy (ClusterFirst) nameserver will
# have a 10.96.0.10 value, which is an IP address of kube-dns Service that load balances TCP and UDP traffic to CoreDNS.
# When using the default ClusterFirst DNS Policy, Kubelet will generate the following /etc/resolv.conf
#
#     search <pod-namespace>.svc.cluster.local svc.cluster.local cluster.local
#     options ndots:5
#
# The search option provides a search list of host-name lookups. DNS queries with fewer dots than specified in the ndots option
# will be attempted using each element of the search field until a match is found. For example, your Pod queries DNS for app.kube-system.
# Then the DNS client checks “1 dot is less than 5 dots” and attempts to lookup -
#     1. app.kube-system.<pod-namespace>.cluster.local.
#     2. app.kube-system.svc.cluster.local.
#     3. app.kube-system.cluster.local.
#     4. app.kube-system.
# Note that if your query ends with a dot, it’s considered a Fully Qualified Domain Name. In this case, the DNS client will resolve
# the DNS entry exactly as it is without using the elements in the search option.
#
# Kubernetes Pod DNS Policy
# Kubernetes additionally supports the following DNS Policies -
#
#     • Default - The Pod inherits the /etc/resolv.conf from the node it runs on. For instance, a CoreDNS Pod typically uses this policy
#       to forward external domain name requests to the node’s nameserver.
#     • ClusterFirstWithHostNet - This policy is for pods with host network enabled, applying ClusterFirst behavior. Otherwise, these pods
#       default to the Default policy.
#     • None - This policy allows custom DNS configurations on the pod spec. Here’s an example:
---
apiVersion: v1
kind: Pod
metadata:
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 10.96.0.10
    searches:
      - suffix1.povilasv.me
      - suffix2.povilasv.me
      - suffix3.povilasv.me
      - kube-system.svc.cluster.local
      - svc.cluster.local
      - cluster.local
    options:
      - name: ndots
        value: "5"
endsnippet

snippet limits "Configure Default/MIN/MAX CPU & Memory Requests & Limits for a Namespace" !b
---
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
spec:
  limits:
  # The default section sets the default limits for a container in a pod.
    - default:
        cpu: 600m
        memory: 100Mi
  # The defaultRequest section sets the default requests for a container in a pod.
      defaultRequest:
        cpu: 100m
        memory: 50Mi
  # The max section will set up the maximum limits that a container in a pod can set.
      max:
        cpu: 1000m
        memory: 200Mi
  # The min section will set up the minimum requests that a container in a pod can set.
      min:
        cpu: 10m
        memory: 10Mi
      type: Container
endsnippet

snippet limits "Configure Default Memory Requests and Limits for a Namespace" !b
# When you add a LimitRange:

# If any Pod in that namespace that includes a container does not specify its own MEMORY limit, the control plane applies
# default MEMORY limit to that container, and the Pod can be allowed to run in a namespace that is restricted by a
# MEMORY ResourceQuota.

---
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
    - default:
        memory: 512Mi
      defaultRequest:
        memory: 256Mi
      type: Container
endsnippet

snippet limits "Configure Default CPU Requests and Limits for a Namespace" !b
# When you add a LimitRange:

# If any Pod in that namespace that includes a container does not specify its own CPU limit, the control plane applies
# the default CPU limit to that container, and the Pod can be allowed to run in a namespace that is restricted by a
# CPU ResourceQuota.

---
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
    - default:
        cpu: 1
      defaultRequest:
        cpu: 0.5
      type: Container
endsnippet

snippet rc "Replication Controller" !b
# WARNING, use Deployments -- ReplicationController is being replaced.
apiVersion: v1
kind: ReplicationController
metadata:
	name: ${1:some-controller}
	namespace: ${2:default}
spec:
	replicas: 1
	template:
		metadata:
			labels:
				app: $1
				tier: $3
		spec:
			containers:
			- name: ${4:name}
				image: ${5:nginx}
				imagePullPolicy: Always
				resources:
					requests:
						cpu: 100m
						memory: 200Mi
				ports:
				- containerPort: 8080
endsnippet

snippet dep "Deployment" !bm
apiVersion: apps/v1
kind: Deployment
metadata:
	name: ${1:some-controller}
	namespace: ${2:default}
spec:
	replicas: 1
	selector:
		matchLabels:
			$3
	template:
		metadata:
			labels:
				${3:app: $1}
		spec:
			containers:
			- name: ${4:name}
				image: ${5:nginx}
				imagePullPolicy: Always
				resources:
					limits:
						cpu: 800m
						memory: 800Mi
					requests:
						cpu: 100m
						memory: 200Mi
				ports:
				- containerPort: 8080
endsnippet

snippet svc "Service ClusterIP (default)" !b
apiVersion: v1
kind: Service
metadata:
	name: ${1:frontend}
	namespace: ${2:default}
	labels:
		app: ${3:someApp}
		tier: ${4:frontend}
spec:
	ports:
	- port: ${5:80}
	selector:
		app: $3
		tier: $4
endsnippet

snippet svc "Service NodePort" !b
apiVersion: v1
kind: Service
metadata:
  name: ${1:frontend}
  namespace: ${2:default}
  labels:
    app: ${3:someApp}
    tier: ${4:frontend}
spec:
  type: NodePort
  selector:
    app.kubernetes.io/name: MyApp
    app: $3
    tier: $4
  ports:
      # By default and for convenience, the 'targetPort' is set to the same value as the 'port' field.
    - port: 80
      targetPort: 80
      # Optional field
      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)
      nodePort: 30007
endsnippet

snippet svc "AWS LoadBalancer" !b
apiVersion: v1
kind: Service
metadata:
  name: ${1:frontend}
  namespace: ${2:default}
  annotations:
    # More details on Annotations here: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/annotations/#annotations
    # Set LoadBalancer Scheme
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"  # Accepted values: 'internet-facing', 'internal'
    # Set LoadBalancer Type
    service.beta.kubernetes.io/aws-load-balancer-type: "classic" # Accepted values: 'classic', 'nlb', 'alb'
    # Set LoadBalancer Name
    service.beta.kubernetes.io/aws-load-balancer-name: "argo-events.elb"
    # Whitelist below IP CIDR's
    service.beta.kubernetes.io/load-balancer-source-ranges: "192.30.252.0/22, 185.199.108.0/22, 140.82.112.0/20, 143.55.64.0/20"
    # Add an ACM Certificate on LB
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: 'arn:aws:acm:us-east-1:998801114546:certificate/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'
    # Resource Tagging
    service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: "purpose=argo-poc"
  labels:
    app: ${3:someApp}
    tier: ${4:frontend}
spec:
  type: LoadBalancer
  selector:
    app: $3
    tier: $4
  ports:
    - port: 80
      targetPort: 8080
      name: http
    - port: 443
      targetPort: 8080
      name: https
endsnippet

snippet svcns "Access Service in Another Namespace" !b
kind: Service
apiVersion: v1
metadata:
  name: service-y
  namespace: namespace-a
spec:
  type: ExternalName
  externalName: service-y.namespace-b.svc.cluster.local
  ports:
  - port: 80

# Use : <service name> (Use if in same namespace)
# Use : <service.name>.<namespace name> (Use if across namespace)
# Use : <service.name>.<namespace name>.svc.cluster.local (FQDN)
# More details: https://stackoverflow.com/questions/37221483/service-located-in-another-namespace
endsnippet

snippet depsvc "Deployment and service" !b
apiVersion: apps/v1
kind: Deployment
metadata:
	name: ${1:some-controller}
	namespace: ${2:default}
spec:
	replicas: 1
	selector:
		matchLabels:
			$3
	template:
		metadata:
			labels:
				${3:app: $1}
		spec:
			containers:
			- name: ${4:name}
				image: ${5:nginx}
				imagePullPolicy: Always
				resources:
					limits:
						cpu: 800m
						memory: 800Mi
					requests:
						cpu: 100m
						memory: 200Mi
				ports:
				- containerPort: ${6:8080}
---
apiVersion: v1
kind: Service
metadata:
	name: $1
	namespace: $2
spec:
	ports:
	- port: ${7:80}
		targetPort: $6
	selector:
		$3
endsnippet

snippet depsvcing "Deployment, service, and ingress" !b
apiVersion: apps/v1
kind: Deployment
metadata:
	name: ${1:some-controller}
	namespace: ${2:default}
spec:
	replicas: 1
	selector:
		matchLabels:
			$3
	template:
		metadata:
			labels:
				${3:app: $1}
		spec:
			containers:
			- name: ${4:name}
				image: ${5:nginx}
				imagePullPolicy: Always
				resources:
					limits:
						cpu: 800m
						memory: 800Mi
					requests:
						cpu: 100m
						memory: 200Mi
				ports:
				- containerPort: ${6:8080}
---
apiVersion: v1
kind: Service
metadata:
	name: $1
	namespace: $2
spec:
	ports:
	- port: ${7:80}
		targetPort: $6
	selector:
		$3
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
	name: $1
	namespace: $2
spec:
	${10:tls:
	- secretName: ${9:$8.tls}
		hosts:
		- $8
	}rules:
	- host: ${8:host}
		http:
			paths:
			- path: ${11:/}
				backend:
					serviceName: $1
					servicePort: $7
endsnippet

snippet pv "PersistentVolume" !b
apiVersion: v1
kind: PersistentVolume
metadata:
	name: ${1:name}
	labels:
		app: ${2:app}
		tier: ${3:tier}
spec:
	capacity:
		storage: ${4:20Gi}
	accessModes:
		- ${5:ReadWriteMany}
	nfs:
		server: ${6:NameOrIP}
		path: ${7:"/share/path/on/server"}
endsnippet

snippet pvc "PersistentVolumeClaim" !b
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: ${1:name}
	labels:
		# insert any desired labels to identify your claim
		app: ${2:app}
		tier: ${3:tier}
spec:
	${4:storageClassName: ${5:standard}}
  # accessModes:
  # • ReadWriteOnce (RWO) - Volume allows read/write by only one node at the same time.
  # • ReadWriteMany (RWX) - Volume allows read/write by multiple nodes at the same time.
  # • ReadOnlyMany  (ROX) - Volume allows read-only mode by many nodes at the same time.
	accessModes:
		- ${6:ReadWriteOnce}
	resources:
		requests:
			# The amount of the volume's storage to request
			storage: ${7:20Gi}
endsnippet

snippet ing "Nginx Ingress Object" !b
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example
  namespace: foo
spec:
  ingressClassName: nginx
  rules:
    - host: www.example.com
      http:
        paths:
          - pathType: Prefix
            backend:
              service:
                name: exampleService
                port:
                  number: 80
            path: /
  # This section is only required if TLS is to be enabled for the Ingress
  # tls:
  #   - hosts:
  #     - www.example.com
  #     secretName: example-tls

## If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided
# ---
# apiVersion: v1
# kind: Secret
# metadata:
#   name: example-tls
#   namespace: foo
# data:
#   tls.crt: <base64 encoded cert>
#   tls.key: <base64 encoded key>
# type: kubernetes.io/tls
endsnippet

snippet ing "Ingress - Exposing 2 services" !b
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-services
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-path: /health
    alb.ingress.kubernetes.io/group.name: "main-elb"
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
          # Custom path prefix
          - path: /service1
            pathType: Prefix
            backend:
              service:
                name: blue-service
                port:
                  number: 8888
          # Custom path prefix
          - path: /service2
            pathType: Prefix
            backend:
              service:
                name: green-service
                port:
                  number: 8888
endsnippet

snippet ing "Ingress - Exposing 2 services with Host Headers" !b
# You can use host conditions to define rules that route requests based on the
# host name in the host header (also known as host-based routing). This enables
# you to support multiple subdomains & different top-level domains using a single load balancer.

# Example request pattern:
# curl --header 'Host: blue.example.com' <string>.<region>.elb.amazonaws.com
# {"message":"I am blue service"}
# curl --header 'Host: green.example.com' <string>.<region>.elb.amazonaws.com
# {"message":"I am GREEN service"}

# Request Flow
# [client] => NLB => Ingress Nginx Controller pods => application pods
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-services
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-path: /health
    alb.ingress.kubernetes.io/group.name: "main-elb"
spec:
  ingressClassName: alb
  rules:
    - host: blue.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: blue-service
                port:
                  number: 8888
    - host: green.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: green-service
                port:
                  number: 8888
endsnippet

snippet ing "Ingress - Ingress nginx rewrite feature to rewrite /service1 to /" !b
# DOCS: https://kubernetes.github.io/ingress-nginx/examples/rewrite/
# Ingress nginx rewrite feature to rewrite /service1 to /
# Furthermore, this configuration will preserve anything after /service1
# for example, if we send /service1/health our backend will receive /health

# • "rewrite.bar.com/service1" rewrites to "rewrite.bar.com/"
# • "rewrite.bar.com/service1/health" rewrites to "rewrite.bar.com/health"

# Example request pattern:
# curl my-services-<string>.elb.<region>.amazonaws.com/service1
# {"message":"I am blue service"}
# curl my-services-<string>.elb.<region>.amazonaws.com/service1/health
# OK

# Request Flow
# [client] => NLB => Ingress Nginx Controller pods => application pods
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: blue-service
  annotations:
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /\$2
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          - pathType: ImplementationSpecific
            path: /service1(/|$)(.*)
            backend:
              service:
                name: blue-service
                port:
                  number: 8888
endsnippet

snippet ns "Namespace" !b
apiVersion: v1
kind: Namespace
metadata:
	name: ${1:name}
endsnippet

snippet sa "ServiceAccount" !b
apiVersion: v1
kind: ServiceAccount
metadata:
	name: ${1:name}
endsnippet

snippet ingtls "Ingress TLS section" !b
tls:
- secretName: ${2:$1.tls}
	hosts:
	- ${1:host}
endsnippet

snippet cfg "ConfigMap" !b
apiVersion: v1
kind: ConfigMap
metadata:
  name: ${1:name}
data:
  # Configuration values can be set as key-value properties
  ${3:key}: ${4:value}
  database: mongodb
  database_uri: mongodb://localhost:27017

  # Or set as complete file contents (even JSON!)
  keys: |
    image.public.key=771
    rsa.public.key=42
endsnippet

snippet cfgr "ConfigMap Reference" !b
apiVersion: v1
kind: Pod
metadata:
  name: your-pod
spec:
  containers:
    - name: env-var-configmap
      image: nginx:1.7.9
      envFrom:
        - configMapRef:
            name: example-configmap
endsnippet

snippet sec "Secret" !b
apiVersion: v1
kind: Secret
metadata:
  name: ${1:secret-name}
type: ${2:Opaque}
data:
  ${3:key}: ${4:value}
endsnippet

snippet env "Environment template" !b
- name: ${1:VAR_NAME}
  value: ${2:value}
endsnippet

snippet pvol "Pod Volume Object"
- name: ${1:name}
	${2:source}:
		name:
endsnippet

snippet job "Kubernetes Job" !b
apiVersion: batch/v1
kind: Job
metadata:
  name: ${1:testjob}
  labels:
    ${2:sometag: somevalue}
spec:
  template:
    metadata:
      name: $1
    spec:
      containers:
        - name: job-testing
          image: ${3:busybox}
          imagePullPolicy: Always
          command:
            - ${4:"override the entrypoint"}
      restartPolicy: ${5:OnFailure|Never}
endsnippet

snippet job "Kubernetes Job with TTL" !b
# In Kubernetes, the TTL (Time-To-Live) mechanism for finished jobs is a feature that helps automatically clean up
# finished Jobs (either Complete or Failed) after a certain period. TTL mechanism provided by a TTL controller.

# This is done to manage resources and avoid clutter in the cluster efficiently. When configuring TTL, it’s important
# to choose a duration that allows sufficient time for any debugging or log retrieval. Set it accordingly as per your use case

# To do this, use the 'ttlSecondsAfterFinished' field in the job manifest. It comes under the spec section.
# It is used to set a time duration (in seconds) after which a completed job becomes eligible for deletion.
# When the TTL controller cleans up the Job, it deletes its dependent objects as well such as Pods.

# Some benefits of using TTL:
#   • TTL automatically removes completed jobs and their resources.
#   • It frees up cluster resources promptly after job completion.
#   • TTL prevents clutter for better cluster management.
#   • It avoids resource overload in environments with frequent job executions.
#   • It helps us to control costs by releasing resources when no longer needed.

# NOTE: TTL controller is typically enabled by default in many Kubernetes setups. If it is not enabled,
# then enable it manually in the Kubernetes controller manager.
---
apiVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  ttlSecondsAfterFinished: 1800
  template:
    metadata:
      name: my-pod
    spec:
      containers:
        - name: my-container
          image: nginx:latest
          command: ["echo", "This is a sample job to show the TTL usecase"]
      restartPolicy: Never
endsnippet

snippet cron "Kubernetes Cronjob" !b
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ${1:name}
spec:
  # Specifies how to treat concurrent executions of a Job. Valid values are:
  # - "Allow" allows CronJobs to run concurrently
  # - "Forbid" forbids concurrent runs, skipping next run if previous hasn't finished yet
  # - "Replace" cancels currently running job and replaces it with a new one
  concurrencyPolicy: Allow
  # Optional deadline in seconds for starting the job if it misses scheduled time for any reason.
  # Missed jobs executions will be counted as failed ones
  startingDeadlineSeconds: 30
  # The number of failed finished jobs to retain. Value must be non-negative integer. Defaults to 1
  failedJobsHistoryLimit: 1
  # The number of successful finished jobs to retain. Value must be non-negative integer. Defaults to 3
  successfulJobsHistoryLimit: 3
  schedule: '${2:*/5} * * * *'
  jobTemplate:
    metadata:
      name: $1
    spec:
      # Specifies the number of retries before marking this job failed. Defaults to 6
      backoffLimit: 3
      template:
        spec:
          containers:
          - name: $1
            image: ${3:busybox}
            imagePullPolicy: Always
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          # Restart policy for all containers within the pod. One of Always, OnFailure, Never
          restartPolicy: OnFailure
endsnippet

snippet skr "SecretKeyRef"
valueFrom:
	secretKeyRef:
		name: ${1:secret-name}
		key: ${2:key-name}
endsnippet

snippet cert "cert-manager certificate" !b
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
	name: ${1:name}
	namespace: ${2:namespace}
spec:
	secretName: ${4:$3.tls}
	dnsNames:
	- ${3:some.domain.com}
	acme:
		config:
		- dns01:
				provider: ${4:prod}
			domains: [ $3 ]
	issuerRef:
		name: ${5:letsencrypt}
		kind: ClusterIssuer
endsnippet

snippet netp "NetworkPolicy" !b
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
	namespace: ${1:default}
	name: ${2:policyname}
	spec:
	${4:podSelector:
			matchLabels:
				${3:{}}
}	ingress:
			- {}
endsnippet

snippet netp "NetworkPolicy (Ingress for pod with label)" !b
# • This NetworkPolicy named example-access-nginx is applied to the "default" namespace
# • It selects Pods with the label "app=nginx" and allows incoming (ingress) traffic from Pods with the label "access=true"
# • This means that only Pods with the label "access=true" can communicate with the selected Pods
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: example-access-nginx
  namespace: default
  # The namespace where the NetworkPolicy is applied
spec:
  podSelector:
    matchLabels:
      app: nginx
      # The NetworkPolicy applies to Pods with the label "app=nginx"
  ingress:
    - from:
        - podSelector:
            matchLabels:
              access: "true"
              # Traffic is allowed from Pods with the label "access=true"
endsnippet

snippet netp "NetworkPolicy (Disable Ingress & Egress traffic in a Namespace)" !b
# To Disable All Ingress Traffic In A Pod Network Space
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  # The name of the NetworkPolicy
spec:
  podSelector: {}
  # No specific Pods are selected by this NetworkPolicy
  # Hence applied to all pods in the namespace where this policy is applied
  policyTypes:
    - Ingress
    # This NetworkPolicy is of type Ingress
    # It effectively denies all incoming traffic to all pods

# To Disable All Egress Traffic In A Pod Network Space
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-egress
  # The name of the NetworkPolicy
spec:
  podSelector: {}
  # No specific Pods are selected by this NetworkPolicy
  # Hence applied to all pods in the namespace where this policy is applied
  policyTypes:
    - Egress
    # This NetworkPolicy is of type Egress
    # It effectively denies all outgoing traffic from all pods
endsnippet

snippet probe "Liveness/Readiness Probes" !b
livenessProbe: &probe
	initialDelaySeconds: ${1:10}
	httpGet:
		port: ${2:8080}
		path: ${3:/}
readinessProbe: *probe $0
endsnippet

snippet ss "StatefulSet" !b
apiVersion: v1
kind: Service
metadata:
	name: ${1:myservice}
spec:
	ports:
	- port: $5
		name: $6
	clusterIP: None
	selector:
		$2
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
	name: ${1:mystatefulset}
spec:
	selector:
		matchLabels:
			$2
	serviceName: "nginx"
	replicas: 3 # by default is 1
	template:
		metadata:
			labels:
				${2:app: $1}
		spec:
			# terminationGracePeriodSeconds: 10
			containers:
			- name: ${3:$1}
				image: ${4:$1}
				ports:
				- containerPort: ${5:80}
					name: ${6:web}
				volumeMounts:
				- name: ${7:volume}
					mountPath: ${8:/var/lib/mydata}
	volumeClaimTemplates:
	- metadata:
			name: $7
		spec:
			accessModes: [ "ReadWriteOnce" ]
			storageClassName: "${9:standard}"
			resources:
				requests:
					storage: ${10:1G}
endsnippet

snippet res "Resources" !b
resources:
	requests:
		cpu: ${1:100m}
		memory: ${2:200Mi}
	${5:limits:
		cpu: ${3:$1}
		memory: ${4:$2}}$0
endsnippet

snippet init "Init Container" !b
initContainers:
- name: ${1:myinit}
	image: ${2:busybox}
	command: [${3:rm, -rf,  $5/lost+found}]
	${6:volumeMounts:
	- name: ${4:data}
		mountPath: ${5:/data}}$0
endsnippet

snippet strat "Deployment Strategy" !b
strategy:
	type: ${1:RollingUpdate|Recreate}
	rollingUpdate:
		maxSurge: ${2:1}
		maxUnavailable: ${3:1}$0
endsnippet

snippet atls "tls-acme annotations" !b
annotations:
	kubernetes.io/tls-acme: "true"
endsnippet

snippet vtls "tls-vault annotations" !b
annotations:
	kubernetes.io/tls-vault: "true"
endsnippet

snippet cmtls "cert-manager tls annotations" !b
${2:annotations:
	}certmanager.k8s.io/cluster-issuer: ${1:lets-encrypt}
endsnippet

snippet edns "external dns" !b
annotations:
	external-dns.alpha.kubernetes.io/hostname: ${1:myname.mydomain.com}
endsnippet

snippet role "Role & Role Binding" !b
# • Role is a way of defining what actions users can perform within a namespace. This defines which actions such as get,
# list, create, delete, etc., can be performed on specific resources like pods, services, and deployments.
# Here, We have permitted pods to get, list, update, delete, and create the resources.
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
  namespace: default
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "list", "update", "delete", "create"]

# • RoleBinding binds the role with one or more users, groups, and service accounts in a particular namespace.
# In this way, whatever permissions we have defined in a role get allotted to the users, groups, or service accounts.
# Below RoleBinding binds the "developer" Role to a user named "dev-user" in the "default" namespace
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
endsnippet

snippet clusterrole "ClusterRole & ClusterRoleBinding" !b
# • ClusterRole is similar to the role, but it is used for cluster-wide permissions. It means it is not limited to a
# specific namespace and can be used in all namespaces of the cluster.
# Here We have permitted nodes to get, list, delete, and create the resources.
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["nodes"]
  verbs: ["get", "list", "delete", "create"]

# • ClusterRoleBinding binds the ClusterRole to one or more users, groups, and service accounts throughout the cluster.
# This allows the permissions defined in ClusterRole to be assigned to users in all namespaces of the cluster.
# Below ClusterRoleBinding binds the "cluster-administrator" ClusterRole to a user named "cluster-admin" in all namespaces
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator
  apiGroup: rbac.authorization.k8s.io
endsnippet

snippet rbac "Role and Binding" !b
kind: ${1:Cluster}Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
	${2:namespace: ${3:default}
	}name: ${4:configmap-updater}
rules:
- apiGroups: ["${5:}"]
	resources: ["${6:configmaps}"]
	resourceNames: ["${7:my-configmap}"]
	verbs: [${8:"update", "get"}]
---
# This role binding allows "jane" to read pods in the "default" namespace.
kind: ${9:Cluster}RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
	name: $4
	$2
subjects:
- kind: ${10:User|ServiceAccount|Group}
	name: ${11:jane} # Name is case sensitive
	apiGroup: rbac.authorization.k8s.io
roleRef:
	kind: $1Role #this must be Role or ClusterRole
	name: $4 # this must match the name of the Role or ClusterRole you wish to bind to
	apiGroup: rbac.authorization.k8s.io
endsnippet

snippet hpa "Horizontal Pod Autoscaler (apiVersion: autoscaling/v1)" !b
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
	name: ${5:$1}
spec:
	scaleTargetRef:
		apiVersion: apps/v1
		kind: Deployment
		name: ${1:mydeployment}
	minReplicas: ${2:1}
	maxReplicas: ${3:5}
	targetCPUUtilizationPercentage: ${4:70}
endsnippet

snippet pvolm "Volume Mount and spec" !b
	volumeMounts:
	- name: ${1:volume}
		mountPath: ${2:/etc/mount/path}
		${3:subPath: ${4: key}}
volumes:
- name: $1
	${5:configMap}:
		${6:name}: ${7:someName}
endsnippet

snippet volm "Volume Mount" !b
- name: ${1:volume}
	mountPath: ${2:/etc/mount/path}
	${3:subPath: ${4: key}}
$0
endsnippet

snippet prom "Prometheus annotations" !b
annotations:
	prometheus.io/scrape: "true"
	prometheus.io/endpoint: "${1:/metrics}"
	prometheus.io/port: "${2:8080}"
endsnippet

snippet aff "Affinitiy/Anti-Affinity" !b
pod${1:Anti}Affinity:
	${2:preferred|required}DuringSchedulingIgnoredDuringExecution:
	- weight: 100
		podAffinityTerm:
			labelSelector:
				matchExpressions:
				- key: ${3:app}
					operator: In
					values:
					- ${4:appname}
			topologyKey: ${5:kubernetes.io/hostname}
endsnippet

snippet hpa "Horizontal Pod Autoscaler (with Stabilization window)" !b
# Stabilization window : Due to the dynamic nature there can be rapid scaling up and down (known as "flapping")
# HPA uses a stabilization window, allowing time for the system to stabilise before making further adjustments
# If not set, use the default values:
# • scale up: 0 (i.e. no stabilization is done)
# • scale down: 300 (i.e. the stabilization window is 300 seconds long)

# type is the type of metric source
# It should be one of "ContainerResource", "External", "Object", "Pods" or "Resource", each mapping to a matching field in the object
# • k explain horizontalPodAutoscaler.spec.metrics
# Note: "ContainerResource" type is available on when the feature-gate HPAContainerMetrics is enabled
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ${1:hpa-name}
  namespace: ${2:default}
spec:
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 200
    scaleDown:
      stabilizationWindowSeconds: 300
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ${3:deployment-name}
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
endsnippet

snippet hpa "Horizontal Pod Autoscaler (Custom Metrics - Ingress Req/s)" !b
# type is the type of metric source
# It should be one of "ContainerResource", "External", "Object", "Pods" or "Resource", each mapping to a matching field in the object
# • k explain horizontalPodAutoscaler.spec.metrics
# Note: "ContainerResource" type is available on when the feature-gate HPAContainerMetrics is enabled
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ${1:hpa-name}
  namespace: ${2:default}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ${3:deployment-name}
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 250
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: main-route
      target:
        type: Value
        value: 10k
endsnippet

snippet hpa "Horizontal Pod Autoscaler (Custom Metrics - Job Queue Length)" !b
# type is the type of metric source
# It should be one of "ContainerResource", "External", "Object", "Pods" or "Resource", each mapping to a matching field in the object
# • k explain horizontalPodAutoscaler.spec.metrics
# Note: "ContainerResource" type is available on when the feature-gate HPAContainerMetrics is enabled
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-application-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-application
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: External
    external:
      metric:
        name: my_queue_length
        selector:
          matchLabels:
            queue: "jobs"
      target:
        type: Value
        value: 10
endsnippet

snippet hpa "Horizontal Pod Autoscaling (Container Resource Metrics)" !b
# Introduced in Kubernetes v1.20 and now graduated to stable in v1.30, the Container resource metrics feature allows HPA to
# target individual container metrics within a pod. You can define the HPA to scale based on the resource utilization
# (CPU, memory, etc.) of a specific container within the pod.
# type is the type of metric source
# It should be one of "ContainerResource", "External", "Object", "Pods" or "Resource", each mapping to a matching field in the object
# • k explain horizontalPodAutoscaler.spec.metrics
# Note: "ContainerResource" type is available on when the feature-gate HPAContainerMetrics is enabled
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: crm-scaling-demo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: crm-scaling-demo
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: ContainerResource # new-metrics-source
    containerResource:
      name: cpu
      container: cpu-stressor # container-name
      target:
        type: Utilization
        averageUtilization: 50
endsnippet

snippet hpa "Horizontal Pod Autoscaling (ArgoRollout Custom Resource)" !b
# You would expect that if the CPU utilisation stay above 70% the Rollout will be increased with
# 4 pods every minute and it will decrease with 1 pod every 15 minutes if the CPU utilisation drop below 70%
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-hpa
spec:
  behavior:
    scaleDown:
      policies:
        - periodSeconds: 900
          type: Pods
          value: 1
      selectPolicy: Min
    scaleUp:
      policies:
        - periodSeconds: 60
          type: Pods
          value: 4
      selectPolicy: Max
  maxReplicas: 48
  minReplicas: 16
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 70
        type: Utilization
    type: Resource
  scaleTargetRef:
    apiVersion: argoproj.io/v1alpha1
    kind: Rollout
    name: my-rollout
endsnippet

snippet vpa "Vertical Pod Autoscaling (Auto-Update Mode)" !b
# Kubernetes VPA Auto-Update Mode
# There are multiple valid options for updateMode in VPA. They are:
# Off       – VPA will only provide the recommendations, and it will not automatically change resource requirements
# Initial   – VPA only assigns resource requests on pod creation and never changes them later
# Recreate  – VPA assigns resource requests on pod creation time and updates them on existing pods by evicting and recreating them
# Auto mode – It recreates the pod based on the recommendation
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: nginx-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: nginx
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: "nginx"
        minAllowed:
          cpu: "250m"
          memory: "100Mi"
        maxAllowed:
          cpu: "500m"
          memory: "600Mi"
endsnippet

snippet vpa "Vertical Pod Autoscaling (Excluding a Container from scaling)" !b
# Excluding Scaling for a Container
# Let’s assume we have a pod running two containers, and we want only one container to scale based on VPA.
# The other container (say a container used to host a performance monitoring agent) should not scale, because
# it does not require scaling.

# We can achieve this by opting out the containers which do not need scaling. In this case, we should
# set mode: "Off" for the container hosting the performance monitoring tool’s agent, which does not require
# the scaling
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-opt-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: my-vpa-deployment
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: newrelic-sidecar-container
        mode: "Off"
endsnippet

snippet vpa "Vertical Pod Autoscaling (Created by Goldilocks - AutoUpdate 'false')" !b
# VPA for argo-rollouts deployment
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  labels:
    creator: Fairwinds
    source: goldilocks
  name: goldilocks-argo-rollouts
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: argo-rollouts
  updatePolicy:
    updateMode: "Off"

# VPA for grafana deployment
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  labels:
    creator: Fairwinds
    source: goldilocks
  name: goldilocks-grafana
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: grafana
  updatePolicy:
    updateMode: "Off"

# VPA for kube-state-metrics deployment
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  labels:
    creator: Fairwinds
    source: goldilocks
  name: goldilocks-kube-state-metrics
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-state-metrics
  updatePolicy:
    updateMode: "Off"
endsnippet

snippet cf_init "Blank CloudFormation Template" !b
AWSTemplateFormatVersion: "2010-09-09"
Description: ${1: Welcome to CFn template!!}
Parameters:
  Parameter1:
    Type: String
    Description: Enter Your Description
    Default: ${2}
    AllowedValues:
      - $2
Resources:
Outputs:
  Output1:
    Description: "Enter Your Description"
    Value: "${3:Enter value to export}"
    Export:
      Name: "${4:Enter your export name}"
endsnippet

snippet cf_param "Define CloudFormation Parameter" !b
Parameter1:
  Type: String
  Description: Enter Your Description
  Default: ${1}
  AllowedValues:
    - $1
endsnippet

snippet cf_output "Define CloudFormation Output" !b
Output1:
  Description: "Enter Your Description"
  Value: "${1:Enter value to export}"
  Export:
    Name: "${2:Enter your export name}"
endsnippet

snippet cf_tags "CloudFormation Resource Tags" !b
Tags:
  - Key: ${1}
    Value: ${2}
endsnippet

snippet ghactions "Github Actions file (Basic)" !b
name: Sample Github Actions File

on:
  push:
    branches:
      - main

  pull_request:
    types:
      - opened

jobs:
  build: # this can be any name you choose
    name: build application
    runs-on: ubuntu-latest
    steps:
      - name: checkout repo code
        uses: actions/checkout@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: List repo files
        run: |
          ls -la
          cat README.md
      - name: Install cowsay
        run: |
          pip install cowsay
          cowsay 'hello'
endsnippet

snippet ghactions "Github Actions file (NodeJS)" !b
name: Build NodeJS Project

on: [push, pull_request]

jobs:
  build: # this can be any name you choose
  # Specify the operating system for the job
  runs-on: ubuntu-latest
  steps:
    # Checkout the repository so that workflow can access the code
    - name: Checkout code
      uses: actions/checkout@v2

    # Set up Node.js environment
    - name: Set up Node.js
      uses: actions/setup-node@v2
      with:
        node-version: '14'

    # Install dependencies using npm
    - name: Install Dependencies
      run: npm ci

    # Build the project
    - name: Build
      run: npm build

    # Run unit tests
    - name: Run Tests
      run: npm test

    # Publish code coverage report using codecov
    - name: Publish Code Coverage
      uses: codecov/codecov-action@v2

    # Deploy the application to a server
    - name: Deploy to Server
      run: ssh user@server 'cd /path/to/project && git pull'
endsnippet

snippet ghactions "Github Actions file (Python)" !b
name: Python package

on: [push]

jobs:
  build: # this can be any name you choose

    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.7", "3.8", "3.9", "3.10", "3.11"]

    steps:
      - uses: actions/checkout@v3
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff pytest
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Lint with ruff
        run: |
          # stop the build if there are Python syntax errors or undefined names
          ruff --format=github --select=E9,F63,F7,F82 --target-version=py37 .
          # default set of ruff rules with GitHub Annotations
          ruff --format=github --target-version=py37 .
      - name: Test with pytest
        run: |
          pytest
endsnippet

snippet ghactions "Github Actions file (Print Github Repo Secrets)" !b
name: Print Github Repository Level Secrets
on:
  workflow_dispatch:
  pull_request:
    branches:
      - main
jobs:
  get-repo-secrets:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
      actions: read
      pull-requests: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref }}
      - name: Echo repository level secrets
        shell: bash
        run: |
          echo ${{ secrets.SLACK_CHANNEL_ID }} | sed 's/./& /g'
          echo ${{ secrets.SLACK_BOT_TOKEN }} | sed 's/./& /g'
          echo ${{ secrets.GCP_SERVICE_ACCOUNT_FEATURE }} | sed 's/./& /g'
          echo ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER_FEATURE }} | sed 's/./& /g'
          echo ${{ secrets.GCP_SERVICE_ACCOUNT_STAGING }} | sed 's/./& /g'
          echo ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER_STAGING }} | sed 's/./& /g'
          echo ${{ secrets.GCP_SERVICE_ACCOUNT_DEMO }} | sed 's/./& /g'
          echo ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER_DEMO }} | sed 's/./& /g'
          echo ${{ secrets.GCP_SERVICE_ACCOUNT_PRODUCTION }} | sed 's/./& /g'
          echo ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER_PRODUCTION }} | sed 's/./& /g'
endsnippet

snippet ghactions "Github Actions file (Python VirtualEnv)" !b
name: Check dependencies

on: [pull_request]

permissions:
  contents: read

jobs:
  check-dependencies:
    name: Check dependencies
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5

      - name: Install dependencies
        run: |
          python -m venv ./venv
          source ./venv/bin/activate
          echo "PATH=${PATH}" >> "${GITHUB_ENV}"
          python -m pip install --upgrade pip
          python -m pip install --requirement requirements.txt
endsnippet

snippet circleci "CircleCI Config file (Basic)" !b
# Use the latest 2.1 version of CircleCI pipeline process engine.
# See: https://circleci.com/docs/configuration-reference
version: 2.1

# Define a job to be invoked later in a workflow.
# See: https://circleci.com/docs/configuration-reference/#jobs
jobs:
  say-hello:
    # Specify the execution environment. You can specify an image from Docker Hub or use one of our convenience images from CircleCI's Developer Hub.
    # See: https://circleci.com/docs/configuration-reference/#executor-job
    docker:
      - image: cimg/base:stable
    # Add steps to the job
    # See: https://circleci.com/docs/configuration-reference/#steps
    steps:
      - checkout
      - run:
          name: "Say hello"
          command: "echo Hello, World!"

# Orchestrate jobs using workflows
# See: https://circleci.com/docs/configuration-reference/#workflows
workflows:
  say-hello-workflow:
    jobs:
      - say-hello
endsnippet

snippet circleci "CircleCI Config file (Concurrent Execution)" !b
version: 2.1

# Define the jobs we want to run for this project
jobs:
  build:
    docker:
      - image: cimg/base:2023.03
    steps:
      - checkout
      - run: echo "this is the build job"
  test:
    docker:
      - image: cimg/base:2023.03
    steps:
      - checkout
      - run: echo "this is the test job"

# Orchestrate our job run sequence
workflows:
  build_and_test:
    jobs:
      - build
      - test
endsnippet

snippet circleci "CircleCI Config file (Sequential Execution)" !b
version: 2.1

# Define the jobs we want to run for this project
jobs:
  build:
    docker:
      - image: cimg/base:2023.03
    steps:
      - checkout
      - run: echo "this is the build job"
  test:
    docker:
      - image: cimg/base:2023.03
    steps:
      - checkout
      - run: echo "this is the test job"

# Orchestrate our job run sequence
workflows:
  build_and_test:
    jobs:
      - build
      - test:
          requires:
            - build
endsnippet

snippet circleci "CircleCI Config file (Approval Workflow)" !b
version: 2.1

# Define the jobs we want to run for this project
jobs:
  build:
    docker:
      - image: cimg/base:2023.03
    steps:
      - checkout
      - run: echo "this is the build job"
  test:
    docker:
      - image: cimg/base:2023.03
    steps:
      - checkout
      - run: echo "this is the test job"
  deploy:
    docker:
      - image: cimg/base:2023.03
    steps:
      - checkout
      - run: echo "this is the deploy job"

# Orchestrate our job run sequence
workflows:
  build_and_test:
    jobs:
      - build
      - test:
          requires:
            - build
      - hold:
          type: approval
          requires:
            - build
            - test
      - deploy:
          requires:
            - hold
endsnippet

snippet gitlab "Gitlab Pipeline Config (BASH)" !b
# You can copy and paste this template into a new `.gitlab-ci.yml` file.
# You should not add this template to an existing `.gitlab-ci.yml` file by using the `include:` keyword.
#
# A full list of available templates can be found here:
# https://docs.gitlab.com/ee/ci/examples/#cicd-templates
# To contribute improvements to CI/CD templates, please follow the Development guide at:
# https://docs.gitlab.com/ee/development/cicd/templates.html
# This specific template is located at:
# https://gitlab.com/gitlab-org/gitlab/-/blob/master/lib/gitlab/ci/templates/Bash.gitlab-ci.yml

# See https://docs.gitlab.com/ee/ci/yaml/index.html for all available options

# you can delete this line if you're not using Docker
image: busybox:latest

before_script:
  - echo "Before script section"
  - echo "For example you might run an update here or install a build dependency"
  - echo "Or perhaps you might print out some debugging details"

after_script:
  - echo "After script section"
  - echo "For example you might do some cleanup here"

build1:
  stage: build
  script:
    - echo "Do your build here"

test1:
  stage: test
  script:
    - echo "Do a test here"
    - echo "For example run a test suite"

test2:
  stage: test
  script:
    - echo "Do another parallel test here"
    - echo "For example run a lint test"

deploy1:
  stage: deploy
  script:
    - echo "Do your deploy here"
  environment: production
endsnippet

snippet ghactions "Github Actions file (Java)" !b
name: Java CI

on: [push]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3
      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'
      - name: Build with Maven
        run: mvn --batch-mode --update-snapshots package
endsnippet

snippet ghactions "Github Actions file (Manual Approval)" !b
name: Github Actions with Manual Approval

on: [workflow_dispatch, pull_request, push]

jobs:
  build:
    name: build
    runs-on: ubuntu-latest
    steps:
      - name: Build
        run: echo building
  deploy:
    name: deploy
    runs-on: ubuntu-latest
    needs: build
    steps:
      - uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.TOKEN }}
          approvers: trstringer,acdunnigan
      - name: Deploy to production
        run: echo deploying
endsnippet

snippet ghactions "Github Actions file (Python - Unit Testing)" !b
name: unit test

on:
  push:
    branches:
      - master
      - detection
    paths:
      - '**.py'
      - '.github/workflows/unit-test.yml'
      - 'environment.yml'
  pull_request:
    branches:
      - master
      - detection
    paths:
      - '**.py'
      - '.github/workflows/unit-test.yml'
      - 'environment.yml'

jobs:
  unit-test:
    name: unit test
    runs-on: ubuntu-latest
    container:
      image: continualai/avalanche-test-${{ matrix.python-version }}:latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.7", "3.8", "3.9", "3.10"]
    defaults:
      run:
        shell: bash -l -c "conda run -n avalanche-env --no-capture-output bash {0}"
    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: python unit test
        id: unittest
        env:
          FAST_TEST: "True"
          USE_GPU: "False"
        run: |
          python -m unittest discover tests &&
          echo "Checking that optional dependencies are not needed" &&
          pip uninstall -y higher ctrl-benchmark torchaudio gym pycocotools lvis &&
          PYTHONPATH=. python examples/eval_plugin.py &&
          echo "Running checkpointing tests..." &&
          bash ./tests/checkpointing/test_checkpointing.sh &&
          echo "Running distributed training tests..." &&
          cd tests &&
          PYTHONPATH=.. python run_dist_tests.py &&
          cd .. &&
          echo "While running unit tests, the following datasets were downloaded:" &&
          ls ~/.avalanche/data
endsnippet

snippet ghactions "Github Actions file (Cron Schedule)" !b
name: Docker Nightly Release

on:
  schedule:
  # Check cron syntax on https://crontab.guru
    - cron: '0 0 * * *'  # everyday at midnight
  push:
    branches:
      - 'master'
    paths:
      - 'docker/nightly/**'
      - '.github/workflows/docker-nightly-release.yml'

jobs:
  docker-nightly-release:
    runs-on: ubuntu-latest
    if: ${{ github.repository == 'ContinualAI/avalanche' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Check date of last commit
        run: |
          let DIFF=(`date +%s -d "1 day ago"`-`git log -1 --pretty=format:%ct`) &&
          echo "date_diff=$DIFF" >> $GITHUB_ENV
      - name: Login to DockerHub
        if: ${{ env.date_diff <= 0 }}  # last commit < 24h ago
        uses: docker/login-action@v1
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PSW }}
      - name: Log in to the Container registry
        if: ${{ env.date_diff <= 0 }}  # last commit < 24h ago
        uses: docker/login-action@v1
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Build and push
        if: ${{ env.date_diff <= 0 }}  # last commit < 24h ago
        uses: docker/build-push-action@v2
        with:
          context: docker/nightly
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/avalanche-nightly:latest
            ghcr.io/continualai/avalanche-nightly:latest
endsnippet

snippet ghactions "Github Actions file (Repository Dispatch event - Raise PR for Helm chart update with Flux)" !b
name: Promotion HR
on:
  repository_dispatch:
    types:
      - HelmRelease/**

permissions:
  contents: write
  pull-requests: write

jobs:
  promote:
    name: promotion
    runs-on: ubuntu-latest
    if: |
      github.event.client_payload.metadata.env == 'dev' &&
      github.event.client_payload.severity == 'info'
    steps:
      # Checkout main branch.
      - uses: actions/checkout@v4
        with:
          ref: dev

      # Parse the event metadata to determine the chart version deployed on dev.
      - name: Get chart version from dev
        id: dev
        run: |
          HELMRELEASE=$(echo ${{ github.event.client_payload.involvedObject.name }})
          NAMESPACE=$(echo ${{ github.event.client_payload.involvedObject.namespace }})
          VERSION=$(echo ${{ github.event.client_payload.metadata.revision }} | cut -d '@' -f1)
          TYPE=$(echo ${{ github.event.client_payload.metadata.type }})
          echo VERSION=${VERSION} >> $GITHUB_OUTPUT
          echo HELMRELEASE=${HELMRELEASE} >> $GITHUB_OUTPUT
          echo NAMESPACE=${NAMESPACE} >> $GITHUB_OUTPUT
          echo TYPE=${TYPE} >> $GITHUB_OUTPUT

      # Patch the chart version in the production Helm release manifest.
      - name: Set chart version in production
        id: production
        env:
          HELM_RELEASE: ${{ steps.dev.outputs.helmrelease }}
          NAMESPACE: ${{ steps.dev.outputs.namespace }}
          CHART_VERSION: ${{ steps.dev.outputs.version }}
          TYPE: ${{ steps.dev.outputs.type }}
        run: |
          echo "Set ${NAMESPACE}/${HELM_RELEASE} chart version to ${CHART_VERSION} in ${TYPE}-stable"
          curv=$(yq '. | select(.metadata.name==env(HELM_RELEASE) and .metadata.namespace==env(NAMESPACE)) | .spec.chart.spec.version' apps/bundles/$TYPE-stable/$TYPE-stable.yaml)
          echo "Current Version: $curv"
          echo "    New Version: $CHART_VERSION"
          if [ -n "$curv" ] && [ "$CHART_VERSION" != "$curv" ]; then
            yq -i '(. | select(.metadata.name == env(HELM_RELEASE) and .metadata.namespace==env(NAMESPACE)) | .spec.chart.spec.version ) = env(CHART_VERSION) ' apps/bundles/$TYPE-stable/$TYPE-stable.yaml
            echo CHANGES=true >> $GITHUB_OUTPUT
          else
            echo "No candidates found"
          fi

      # Open a Pull Request if an upgraded is needed in production.
      - name: Open promotion PR
        uses: peter-evans/create-pull-request@v6
        if: ${{ steps.production.outputs.changes }}
        with:
          branch: dev-promotion-${{ github.run_number }}
          delete-branch: true
          token: ${{ secrets.CICD_GITHUB_TOKEN }}
          commit-message: Update ${{ steps.dev.outputs.helmrelease }} to v${{ steps.dev.outputs.version }}
          title: "feat: Promote ${{ steps.dev.outputs.namespace }}/${{ steps.dev.outputs.helmrelease }} release to ${{ steps.dev.outputs.version }} in ${{ steps.dev.outputs.type }}-stable"
          body: |
            **Automated PR**
            HelmRelease ${{ steps.dev.outputs.namespace }}/${{ steps.dev.outputs.helmrelease }} was upgraded to version ${{ steps.dev.outputs.version }} in ${{ steps.dev.outputs.type }}-flex.
            Promote to stable.
endsnippet

snippet pod "Check Internet Connection with Pod in default NS" !b
---
apiVersion: v1
kind: Pod
metadata:
  name: http-check
  namespace: default
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    supplementalGroups: [0]
  containers:
    - name: check-http-connection
      image: busybox:1.37.0
      command:
        - sh
        - -cx
        - |
          #!/usr/bin/env bash -e
          # Download kubernetes.io in /tmp/index.html [Log server responses + Redirections; bypass certificate validation]
          wget -S -P /tmp --no-check-certificate www.kubernetes.io
          # Run webserver to serve /tmp/index.html on port 8080 in foreground [verbosity enabled]
          httpd -f -p 8080 -h /tmp/ -v
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
      resources:
        limits:
          memory: '600Mi'
          cpu: '500m'
        requests:
          memory: '400Mi'
          cpu: '300m'
      livenessProbe:
        initialDelaySeconds: 5
        periodSeconds: 5
        timeoutSeconds: 1
        successThreshold: 1
        failureThreshold: 1
        httpGet:
          host:
          scheme: HTTP
          path: /
          httpHeaders:
          - name: Host
            value: localhost
          port: 8080
      readinessProbe:
        exec:
          command:
            - cat
            - /etc/hostname
endsnippet

snippet pod "Pod Spec honouring IndexExchange's Admission Webhook Rules" !b
---
apiVersion: v1
kind: Pod
metadata:
  name: index-test
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    supplementalGroups: [0]
  containers:
    - name: index-test
      image: registry.indexexchange.com:5000/python:3.6-alpine
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          python -m http.server 8080 &
          while true ; do sleep 5 && echo "$(date) -> Hello from pod $(hostname)" ; done
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
      resources:
        limits:
          memory: '600Mi'
          cpu: '500m'
        requests:
          memory: '400Mi'
          cpu: '300m'
      livenessProbe:
        initialDelaySeconds: 5
        periodSeconds: 5
        timeoutSeconds: 1
        successThreshold: 1
        failureThreshold: 1
        httpGet:
          host:
          scheme: HTTP
          path: /
          httpHeaders:
          - name: Host
            value: localhost
          port: 8080
      readinessProbe:
        exec:
          command:
            - cat
            - /etc/os-release
      securityContext:
        readOnlyRootFilesystem: true
endsnippet

snippet pod "Security Hardened Pod manifest" !b
---
# Define the API version for the Pod object
apiVersion: v1
# Specify the kind of Kubernetes object being defined
kind: Pod
# Metadata block for the Pod object
metadata:
  # Name of the Pod object
  name: nginx
  # Namespace where the Pod will be deployed; not default
  namespace: custom
  # Labels assigned to the Pod object
  labels:
    # Label indicating the application name
    app: nginx
# Specification block for the Pod object
spec:
  # Security context for the Pod
  securityContext:
    # List of supplementary groups to add to the init container's group IDs
    supplementalGroups: [10001]
    # Type of seccomp profile to apply
    seccompProfile:
      type: "RuntimeDefault"
  # Containers to be run inside the Pod
  containers:
    - name: nginx
      # Pull policy for the container image
      imagePullPolicy: Always
      # Specific image to use for the container, pinned by digest for enhanced security
      image: index.docker.io/library/nginx@sha256:6af79ae5de407283dcea8b00d5c37ace95441fd58a8b1d2aa1ed93f5511bb18c
      # Ports exposed by the container
      ports:
        - containerPort: 80
      # Volume mounts for the container
      volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
      # Resource limits and requests for the container
      resources:
        limits:
          memory: '1Gi'
          cpu: '800m'
        requests:
          memory: '700Mi'
          cpu: '400m'
      # Security context for the container
      # NOTE: Field values of container.securityContext take precedence over field values of PodSecurityContext
      securityContext:
        # Run the container as a specific user ID
        runAsUser: 10001
        # Run the container as a specific group ID
        runAsGroup: 10001
        # Prevent the container from gaining root privileges
        runAsNonRoot: true
        # Mount the container's filesystem as read-only
        readOnlyRootFilesystem: true
        # Do not grant extended privileges to the container
        privileged: false
        # Disable privilege escalation
        allowPrivilegeEscalation: false
        # Capabilities to drop from the container
        capabilities:
          drop:
            - all
        # Seccomp profile type
        seccompProfile:
          type: "RuntimeDefault"
  # Init containers to run before the main containers
  initContainers:
    - name: install
      # Pull policy for the init container image
      imagePullPolicy: Always
      # Specific image to use for the init container, pinned by digest for enhanced security
      image: index.docker.io/library/busybox@sha256:9ae97d36d26566ff84e8893c64a6dc4fe8ca6d1144bf5b87b2b85a32def253c7
      # Command to execute in the init container
      command:
        - wget
        - '-O'
        - '/work-dir/index.html'
        - http://kubernetes.io
      # Volume mounts for the init container
      volumeMounts:
        - name: workdir
          mountPath: '/work-dir'
      # Resource limits and requests for the init container
      resources:
        limits:
          cpu: '300m'
          memory: '500Mi'
        requests:
          cpu: '150m'
          memory: '200Mi'
      # Security context for the init container
      securityContext:
        # Run the init container as a specific user ID
        runAsUser: 10001
        # Run the init container as a specific group ID
        runAsGroup: 10001
        # Prevent the init container from gaining root privileges
        runAsNonRoot: true
        # Mount the init container's filesystem as read-only
        readOnlyRootFilesystem: true
        # Do not grant extended privileges to the init container
        privileged: false
        # Disable privilege escalation for the init container
        allowPrivilegeEscalation: false
        # Capabilities to drop from the init container
        capabilities:
          drop:
            - all
        # Seccomp profile type for the init container
        seccompProfile:
          type: "RuntimeDefault"
  # DNS policy for the Pod
  dnsPolicy: Default
  # Volumes to be mounted into the Pod
  volumes:
    - name: workdir
      emptyDir: {}
endsnippet

snippet jira "jira ticket template at IndexExchange" !b
update:
  comment:
    - add:
        body: |~

fields:
  summary: >-
    # TODO: Add Issue summary here
  components: # Values: ACL, Ad-hoc, Aerospike, Akamai, Alert Documentation, Ansible, app/demand-data-api, app/reporting/dag/deployment/reporting-api, app/reporting/dag/deployment/telegraf-mysql-exporter, APPMISC, APPRULES, APPSTATS, architecture/edge-roadmap/legacy-flush, architecture/edge-roadmap/loadbalancing, architecture/edge-roadmap/vdc, architecture/event-driven/async-rtb-impressions, architecture/performance/aerospike, architecture/privacy-sandbox, architecture/project-cassandra/capacity/edge, architecture/project-cassandra/data-platform, architecture/project-cassandra/databases/aerospike, architecture/project-cassandra/databases/mongodb, architecture/project-cassandra/databases/vertica, architecture/project-cassandra/databases/viper2, architecture/project-cassandra/dc-wide-failure, architecture/project-cassandra/ransomware, architecture/project-cassandra/sdlc-toolchain, architecture/project-cassandra/ssh-security, architecture/project-cassandra/ssl-management, ArgoCD, Billing, Blackhole (Hadoop), Blackhole2 (Hadoop), BLH, Ceph, Ceph Platform, Cloudflare, Cobbler, Cohort Proxy, DB, DCAPP, DCSTAT, Dell OpenManage, DNS, DW, ELK Stack, Exchange Node, exchange-node/deployment/impression-service-telegraf, exchange-node/rules/monorepo, exchange-node/supply, features/bto_vdc_override_enabled, features/enable_metric_recording, Filedump2, Gitlab, GLP, gprofiler, Grafana, HAProxy, Image Servers, Jenkins, Jira, k8s-dapr, Kafka, Kepler (Vertica), Kubernetes, L2 API, ldap, Legacy Docker, LibreNMS, LibreNMS PoC, liquibase, MAAS, MAAS provisioning, Mimir, MongoDB, MX1, N/A, NetBox, NetScaler, Network, Nexus, Nexus-IQ, none, novus/hubble-common, novus/reporting-vertica, novus/trino-vertica-connector, NS1, OpenLB, OpenNebula, OS platform, Perflab, PINC Process, Pithos (MariaDB, Galera), PostgreSQL, Prometheus, ptv/event-tracker-deployment, Redis, Request Monitor, SFTP, Sidecar, SonarQube, stor, Storage, sw-edge, Tardis InfluxDB, Telegraf, TLS, Trino, UA, Vault, Viper2 (MySQL), Virtualization, VMware, VPN, Warn Server, Zabbix,
    - name: # TODO: Add component value here
  reporter:
    name: amaan.khan  # TODO: Check your username from https://jira.indexexchange.com/rest/api/2/myself
    emailAddress: amaan.khan@indexexchange.com
  priority: # Values: Urgent, High, Normal, Low,
    name: # TODO: Add priority value here
  description: |~
    # TODO: Add a description here

    *Reference Code*
    {code:bash}
    # TODO: Insert some reference code here
    {code}

    *Reference links*
    # TODO: Add reference links here
    • [Link1|https://example.com]
    • [Link2|https://example.com]

# votes: 0
# comments:
endsnippet

snippet jira "jira ticket template at IndexExchange for DSP Adspend alert (DEM project)" !b
# Should be created in DEM project
# Example: # issue: DEM-14168 - created: 3 hours ago
fields:
  summary: >-
    # TODO: Add Issue summary here
    # Example: NOC-Media Smart- DSP 187 Ad spend Alert: FR - November 12, 2024
  reporter:
    name: amaan.khan  # TODO: Check your username from https://jira.indexexchange.com/rest/api/2/myself
    emailAddress: amaan.khan@indexexchange.com
  priority: # Values: Urgent, High, Normal, Low,
    name: Normal
  labels:
    name: DSP, Support, spendflagNOC
  description: |~
    # TODO: Add a description here

    *Reference Code*
    {code:bash}
    # TODO: Insert some reference code here
    {code}

# votes: 0
# comments:
endsnippet

snippet app "ArgoCD App Manifest (Category 1)" !b
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  # Name of application
  name: guestbook
  namespace: argocd
spec:
  # Project is a reference to the project this application belongs to.
  # The empty string means that application belongs to the 'default' project
  project: default
  source:
    # Where to read the Kubernetes manifest from?
    repoURL: https://github.com/argoproj/argocd-example-apps.git
    targetRevision: HEAD
    path: guestbook
  # Destination is a reference to the target Kubernetes server and namespace
  destination:
    # Which cluster to deploy the application to?
    server: https://kubernetes.default.svc
    namespace: guestbook
endsnippet


snippet app "ArgoCD App Manifest Helm Templating - Anti-pattern" !b
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-helm-override
  namespace: argocd
spec:
  # Project is a reference to the project this application belongs to.
  # The empty string means that application belongs to the 'default' project
  project: default

  source:
    repoURL: https://github.com/example-org/example-repo.git
    targetRevision: HEAD
    path: my-chart

    helm:
      # DON'T DO THIS
      parameters:
        - name: "my-example-setting-1"
          value: my-value1
        - name: "my-example-setting-2"
          value: "my-value2"
          forceString: true   # ensures that value is treated as a string

      # DON'T DO THIS
      values: |
        ingress:
          enabled: true
          path: /
          hosts:
            - mydomain.example.com

      # DON'T DO THIS
      valuesObject:
        image:
          repository: docker.io/example/my-app
          tag: 0.1
          pullPolicy: IfNotPresent
  # Destination is a reference to the target Kubernetes server and namespace
  destination:
    server: https://kubernetes.default.svc
    namespace: my-app
endsnippet

snippet app "ArgoCD App Manifest Helm Templating - Recommended (Category 2)" !b
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-helm-override
  namespace: argocd
spec:
  # Project is a reference to the project this application belongs to.
  # The empty string means that application belongs to the 'default' project
  project: default

  source:
    repoURL: https://github.com/example-org/example-repo.git
    targetRevision: HEAD
    path: my-chart

    helm:
      ## DO THIS (values in Git on their own)
      valueFiles:
        - values-production.yaml
  # Destination is a reference to the target Kubernetes server and namespace
  destination:
    server: https://kubernetes.default.svc
    namespace: my-app
endsnippet

snippet app "ArgoCD App Manifest Helm Templating with External Charts" !b
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-proper-helm-app
  namespace: argocd
spec:
  # Project is a reference to the project this application belongs to.
  # The empty string means that application belongs to the 'default' project
  project: default
  sources:
    - repoURL: 'https://my-chart/helm-charts'
      chart: my-helm-chart
      targetRevision: 3.7.1
      helm:
        valueFiles:
          - $values/my-chart-values/values-prod.yaml
    ## DO THIS (values in Git on their own)
    - repoURL: 'https://git.example.com/org/value-files.git'
      targetRevision: dev
      ref: values
  # Destination is a reference to the target Kubernetes server and namespace
  destination:
    server: https://kubernetes.default.svc
    namespace: my-app
endsnippet

snippet app "ArgoCD App Manifest Kustomize Templating - Anti-pattern" !b
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-kustomize-override
  namespace: argocd
spec:
  # Project is a reference to the project this application belongs to.
  # The empty string means that application belongs to the 'default' project
  project: default
  source:
    repoURL: https://github.com/example-org/example-repo.git
    targetRevision: HEAD
    path: my-app

    # DON'T DO THIS
    kustomize:
      namePrefix: prod-
      images:
        - docker.io/example/my-app:0.2
      namespace: custom-namespace

  # Destination is a reference to the target Kubernetes server and namespace
  destination:
    server: https://kubernetes.default.svc
    namespace: my-app
endsnippet

snippet app "ArgoCD App Manifest Kustomize Templating - Recommended (Category 2)" !b
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-proper-kustomize-app
  namespace: argocd
spec:
  # Project is a reference to the project this application belongs to.
  # The empty string means that application belongs to the 'default' project
  project: default
  source:
    repoURL: https://github.com/example-org/example-repo.git
    targetRevision: HEAD
    ## DO THIS. Save all values in the Kustomize Overlay itself
    path: my-app/overlays/prod
  # Destination is a reference to the target Kubernetes server and namespace
  destination:
    server: https://kubernetes.default.svc
    namespace: my-app
endsnippet

snippet appset "ArgoCD AppSet Manifest Example" !b
# This generator says “take all the apps under application-sets/example-apps and deploy them to all clusters currently
# defined in Argo CD”. It doesn’t matter how many clusters are currently connected or how many applications exist in
# the Git repo. The Application Set generator will automatically create all the possible combinations and also
# continuously redeploy as you add new clusters or new applications.

apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: cluster-git
spec:
  generators:
    # matrix 'parent' generator
    - matrix:
        generators:
          # Git generator, 'child' #1
          # https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Git
          - git:
              repoURL: https://github.com/codefresh-contrib/gitops-cert-level-2-examples.git
              revision: HEAD
              directories:
                - path: application-sets/example-apps/*
          # Cluster generator, 'child' #2
          # https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Cluster
          - clusters: {}
  template:
    metadata:
      name: '{{path.basename}}-{{name}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/codefresh-contrib/gitops-cert-level-2-examples.git
        targetRevision: HEAD
        path: '{{path}}'
      destination:
        server: '{{server}}'
        namespace: '{{path.basename}}'
endsnippet

snippet appproj "ArgoCD AppProject Manifest Example (redis-cluster)" !b
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: redis-cluster
  namespace: argocd
  labels:
    inherit-global: "true"
spec:
  # Description contains optional project description
  description: Redis Cluster resources
  # Destinations contains list of destinations available for deployment
  destinations:
    # Namespace specifies the target namespace for the application's resources
    - namespace: mlo-inference
      # Server specifies the URL of the target cluster and must be set to the Kubernetes control plane API
      server: '*'
  # SourceRepos contains list of repository URLs which can be used for deployment
  sourceRepos:
    - 'https://gitlab.indexexchange.com/operations/redis/k8s-deployment/bitnami-redis-cluster.git'
  # Roles are user defined RBAC roles associated with this project
  roles:
    - description: "Access to view applications"
      # Groups are a list of OIDC group claims bound to this role
      groups:
        - Cloud Platform
        - Architecure
      # Name is a name for this role
      name: view
      # Policies Stores a list of casbin formatted strings that define access policies for the role in the project
      policies:
        - p, proj:redis-cluster:view, applications, get, redis-cluster/*, allow
    - description: "Access to deploy applications"
      # Groups are a list of OIDC group claims bound to this role
      groups:
        - Cloud Platform
        - Architecure
      name: deploy
      # Policies Stores a list of casbin formatted strings that define access policies for the role in the project
      policies:
        - p, proj:redis-cluster:deploy, applications, get, redis-cluster/*, allow
        - p, proj:redis-cluster:deploy, applications, sync, redis-cluster/*, allow
        - p, proj:redis-cluster:deploy, applications, create, redis-cluster/*, allow
        - p, proj:redis-cluster:deploy, applications, update, redis-cluster/*, allow
        - p, proj:redis-cluster:deploy, applications, delete, redis-cluster/*, allow
endsnippet

snippet appproj "ArgoCD AppProject Manifest Example (Multiple Operators)" !b
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  labels:
    argocd.argoproj.io/instance: projects
    inherit-global: "true"
  name: operators
  namespace: argocd
spec:
  # ClusterResourceWhitelist contains list of whitelisted cluster level resources
  clusterResourceWhitelist:
    - group: rbac.authorization.k8s.io
      kind: ClusterRole
    - group: rbac.authorization.k8s.io
      kind: ClusterRoleBinding
    - group: apiextensions.k8s.io
      kind: CustomResourceDefinition
  # Description contains optional project description
  description: Operators managed by Cloud Platform
  # Destinations contains list of destinations available for deployment
  destinations:
    # Namespace specifies the target namespace for the application's resources
    - namespace: operators
      # Server specifies the URL of the target cluster and must be set to the Kubernetes control plane API
      server: '*'
    - namespace: advsearch-es
      server: '*'
  # Roles are user defined RBAC roles associated with this project
  roles:
    - description: Access to view applications
      # Groups are a list of OIDC group claims bound to this role
      groups:
        - Cloud Platform
      name: view
      # Policies Stores a list of casbin formatted strings that define access policies for the role in the project
      policies:
        - p, proj:operators:view, applications, get, operators/*, allow
    - description: Access to deploy applications
      # Groups are a list of OIDC group claims bound to this role
      groups:
        - Cloud Platform
      name: deploy
      # Policies Stores a list of casbin formatted strings that define access policies for the role in the project
      policies:
        - p, proj:operators:deploy, applications, get, operators/*, allow
        - p, proj:operators:deploy, applications, sync, operators/*, allow
        - p, proj:operators:deploy, applications, create, operators/*, allow
        - p, proj:operators:deploy, applications, update, operators/*, allow
        - p, proj:operators:deploy, applications, delete, operators/*, allow
        - p, proj:operators:deploy, exec, create, operators/*, allow
  # SourceRepos contains list of repository URLs which can be used for deployment
  sourceRepos:
    # Deploys ElasticSearch Operator
    - https://gitlab.indexexchange.com/operations/eck-operator.git
    # Deploys Zalando PostgreSQL operator (https://github.com/zalando/postgres-operator)
    - https://gitlab.indexexchange.com/operations/k8s/postgresql/operator.git
endsnippet

snippet pod "Pod manifest for GPU requests" !b
# How can you enable GPUs on a Kubernetes Pod?
# It comes down to:
#     ✅ The resource/limits map
#     ✅ The Node Selector
# You don't actually need to use the Node Selector, but if you don't, your Pod will get whatever GPU node is up for scheduling.
# If you want to use a specific GPU for your workload, you'll want to use the Node Selector so you can choose which GPU you want
# to use. The resource/limits map tells Kubernetes how many GPUs you want for your workload.

apiVersion: v1
kind: Pod
metadata:
  name: nginx-deployment
spec:
  # Assign priority levels to pods, which determines how they are scheduled and evicted
  priorityClassName: system-node-critical
  containers:
    - name: ubuntu2004
      image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04
      resources:
        limits:
          nvidia.com/gpu: 1
  nodeSelector:
    nvidia.com/gpu.product: H100-PCIE-80GB
endsnippet

snippet gitlab "Gitlab variables file for Armada pipeline - Upgrade a Node (IndexExchange)" !b
[
  {
    "key": "ANSIBLE_LIMIT",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_ADD_WORKER_NODE_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_BUILD_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_POST_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_PRE_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_REMOVE_NODE_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_UPGRADE_EXTRA_ARGS",
    "value": "${2:--skip-tags=multus,pre-upgrade,node-taint -e serial=1 --limit k8sintw187.tor3.indexww.com}",
    "variable_type": "env_var"
  },
  {
    "key": "IX_HOSTNAME_PATTERN",
    "value": "${1:Ansible Group Name pattern for K8s cluster from inventory; example: k8sint.*}",
    "variable_type": "env_var"
  },
  {
    "key": "NODE",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "NODE_STATUS",
    "value": "ONLINE",
    "variable_type": "env_var"
  }
]
endsnippet

snippet gitlab "Gitlab variables file for Armada pipeline - Remove a Node (IndexExchange)" !b
[
  {
    "key": "ANSIBLE_LIMIT",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_ADD_WORKER_NODE_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_BUILD_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_POST_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_PRE_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_REMOVE_NODE_EXTRA_ARGS",
    "value": "${2:--limit k8sintw187.tor3.indexww.com}",
    "variable_type": "env_var"
  },
  {
    "key": "CLUSTER_UPGRADE_EXTRA_ARGS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "IX_HOSTNAME_PATTERN",
    "value": "${1:Ansible Group Name pattern for K8s cluster from inventory; example: k8sint.*}",
    "variable_type": "env_var"
  },
  {
    "key": "NODE",
    "value": "k8sintw187.tor3.indexww.com",
    "variable_type": "env_var"
  },
  {
    "key": "NODE_STATUS",
    "value": "ONLINE",
    "variable_type": "env_var"
  }
]
endsnippet

snippet gitlab "Gitlab Pipeline for Cobbler Installation (IndexExchange)" !b
[
  {
    "key": "COBBLER_OPTIONS",
    "value": "",
    "variable_type": "env_var"
  },
  {
    "key": "COBBLER_SERVER",
    "value": "cobbler.up.win.indexww.com",
    "variable_type": "env_var"
  },
  {
    "key": "FILE",
    "value": "blackhole2-stg-data-41.win.indexww.com 10.6.201.55 14:18:77:3b:2c:ad 10.4.0.252",
    "variable_type": "env_var"
  },
  {
    "key": "INTERFACE",
    "value": "em1",
    "variable_type": "env_var"
  },
  {
    "key": "NAMESERVERS",
    "value": "10.4.0.252",
    "variable_type": "env_var"
  },
  {
    "key": "NETMASK",
    "value": "255.252.0.0",
    "variable_type": "env_var"
  },
  {
    "key": "PROFILE",
    "value": "SM15-NOVUS-DATA",
    "variable_type": "env_var"
  },
  {
    "key": "WORKDIR",
    "value": "/tmp/OPD-XXXX",
    "variable_type": "env_var"
  }
]
endsnippet

snippet rollout "Argo Rollout App Manifest - BlueGreen Strategy" !b
# Argo Rollout spec is similar to the deployment object, the Argo Rollouts controller will manage the creation, scaling, and
# deletion of ReplicaSets. These ReplicaSets are defined by the spec.template field inside the Rollout resource, which uses
# the same pod template as the deployment object.

# • A Blue Green Deployment allows users to reduce the amount of time multiple versions running at the same time.
---
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: rollout-bluegreen
spec:
  replicas: 2
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: rollout-bluegreen
  template:
    metadata:
      labels:
        app: rollout-bluegreen
    spec:
      containers:
      - name: rollouts-demo
        image: argoproj/rollouts-demo:blue
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
  strategy:
    blueGreen: # Indicates that the rollout should use the BlueGreen strategy
      # activeService specifies the service to update with the new template hash at time of promotion.
      # This field is mandatory for the blueGreen update strategy.
      activeService: rollout-bluegreen-active
      # previewService specifies the service to update with the new template hash before promotion.
      # This allows the preview stack to be reachable without serving production traffic.
      # This field is optional.
      previewService: rollout-bluegreen-preview
      # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout
      # immediately before the promotion. If omitted, the default behavior is to promote the new
      # stack as soon as the ReplicaSet are completely ready/available.
      # Rollouts can be resumed using: `kubectl argo rollouts promote ROLLOUT`
      autoPromotionEnabled: false

# DOC: https://argo-rollouts.readthedocs.io/en/stable/features/bluegreen
# Here are the optional fields that will change the behavior of BlueGreen deployment:
# spec:
#   strategy:
#     blueGreen:
#       # Defaults to true
#       autoPromotionEnabled: boolean
#       # Defaults to nil
#       autoPromotionSeconds: *int32
#       # Defaults to nil
#       antiAffinity: object
#       # Defaults to an empty string
#       previewService: string
#       # Defaults to nil
#       prePromotionAnalysis: object
#       # Defaults to nil
#       postPromotionAnalysis: object
#       # The PreviewReplicaCount field will indicate the number of replicas that the new version of an application should run
#       previewReplicaCount: *int32
#       # Defaults to 30
#       scaleDownDelaySeconds: *int32
#       # The ScaleDownDelayRevisionLimit limits the number of old active ReplicaSets to keep scaled up while they wait for the
#       # scaleDownDelay to pass after being removed from the active service.
#       scaleDownDelayRevisionLimit: *int32
endsnippet

snippet rollout "Argo Rollout App Manifest - Canary Strategy (Simple)" !b
# Argo Rollout spec is similar to the deployment object, the Argo Rollouts controller will manage the creation, scaling, and
# deletion of ReplicaSets. These ReplicaSets are defined by the spec.template field inside the Rollout resource, which uses
# the same pod template as the deployment object.
# • A canary rollout is a deployment strategy where the operator releases a new version of their application to a small percentage
#   of the production traffic.
---
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: example-rollout
spec:
  replicas: 10
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.4
        ports:
        - containerPort: 80
  minReadySeconds: 30
  revisionHistoryLimit: 3
  strategy:
    canary: # Indicates that the rollout should use the Canary strategy
      maxSurge: "25%"
      maxUnavailable: 0
      steps:
      - setWeight: 10
      - pause:
          duration: 1h # 1 hour
      - setWeight: 20
      - pause: {} # pause indefinitely

# DOC: https://argo-rollouts.readthedocs.io/en/stable/features/canary
# Here are the optional fields that will modify the behavior of canary strategy:
# spec:
#   strategy:
#     canary:
#       # Defaults to nil
#       analysis: object
#       # Defaults to nil
#       antiAffinity: object
#       # Defaults to an empty string
#       canaryService: string
#       # Defaults to an empty string
#       stableService: string
#       # Defaults to "25%"
#       maxSurge: stringOrInt
#       # Defaults to "25%"
#       maxUnavailable: stringOrInt
#       # Defaults to nil
#       trafficRouting: object
endsnippet

snippet rollout "Argo Rollout App Manifest - Canary Strategy (Traffic Routing via Nginx Ingress)" !b
# Argo Rollout spec is similar to the deployment object, the Argo Rollouts controller will manage the creation, scaling, and
# deletion of ReplicaSets. These ReplicaSets are defined by the spec.template field inside the Rollout resource, which uses
# the same pod template as the deployment object.
# • A canary rollout is a deployment strategy where the operator releases a new version of their application to a small percentage
#   of the production traffic.
# DOCS: https://argo-rollouts.readthedocs.io/en/stable/features/traffic-management/nginx/
---
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: example-rollout
spec:
  replicas: 10
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.4
        ports:
        - containerPort: 80
  minReadySeconds: 30
  revisionHistoryLimit: 3
  strategy:
    canary:
      canaryService: canary-service  # required
      stableService: stable-service  # required
      trafficRouting:
        nginx:
          # Either stableIngress or stableIngresses must be configured, but not both
          stableIngress: primary-ingress
          stableIngresses:
            - primary-ingress
            - secondary-ingress
            - tertiary-ingress
          annotationPrefix: customingress.nginx.ingress.kubernetes.io # optional
          additionalIngressAnnotations:   # optional
            canary-by-header: X-Canary
            canary-by-header-value: iwantsit
      # delay scaling down the old ReplicaSet after the active Service is switched to the new ReplicaSet
      scaleDownDelaySeconds: 30
      steps:
      # Set Canary service to scale of 3 pods initially
      - setCanaryScale:
          replicas: 3
      - setWeight: 20
      - pause:
          duration: 10s
      - setWeight: 40
      - pause:
          duration: 10s
      - setWeight: 60
      - pause:
          duration: 10s
      - setWeight: 80
      - pause:
          duration: 10s
      - setWeight: 100
endsnippet

snippet pc "PriorityClass Manifest" !b
# This YAML manifest defines a PriorityClass resource in Kubernetes. It's used to assign priority levels to pods, which
# determines how they are scheduled and evicted
# This PriorityClass is likely used for system-critical pods that should remain on their current node unless absolutely necessary.
# The high priority value ensures these pods are not easily evicted or rescheduled.
---
apiVersion: scheduling.k8s.io/v1
description: "Used for system critical pods that must not be moved from their current node"
kind: PriorityClass
metadata:
  name: system-node-critical
  namespace: default
# Policy for handling preemption requests
preemptionPolicy: PreemptLowerPriority
# Value representing the priority level (higher values indicate higher priority)
value: 2000001000
endsnippet

snippet flux-gitrepo "Flux GitRepository with Kustomization (Install velero)" !b
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: velero-crd-repo
  namespace: flux-system
spec:
  interval: 5m
  url: https://github.com/vmware-tanzu/helm-charts
  ref:
    tag: velero-8.1.0
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: velero-crd
  namespace: flux-system
spec:
  interval: 1m
  targetNamespace: flux-system
  sourceRef:
    kind: GitRepository
    name: velero-crd-repo
  path: "./charts/velero/crds"
  prune: true
  timeout: 1m
endsnippet

snippet flux-gitrepo "Flux GitRepository (Install VictoriaMetrics HelmChart as GitRepo)" !b
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: victoria-metrics
  namespace: flux-system
spec:
  interval: 24h
  url: https://github.com/VictoriaMetrics/helm-charts
  ref:
    branch: master
  ignore: |
    # exclude all
    /*
    # include deploy dir
    !/charts/victoria-metrics-operator/charts/crds
endsnippet

snippet flux-helmrelease "Flux HelmRelease CRD (Install VictoriaMetrics)" !b
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics-crds
  namespace: flux-system
spec:
  interval: 60m
  driftDetection:
    mode: enabled
  chart:
    spec:
      # Name of the Helm chart to use
      chart: ./charts/victoria-metrics-operator/charts/crds
      reconcileStrategy: Revision
      sourceRef:
        kind: GitRepository
        name: victoria-metrics
        namespace: flux-system
      # Interval between automatic updates of the chart (5 minutes)
      interval: 5m
  install:
    crds: CreateReplace
  upgrade:
    crds: CreateReplace
endsnippet

snippet flux-helmrelease "Flux HelmRelease CRD (Nginx Ingress - CIS Compliant)" !b
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
spec:
  targetNamespace: ingress-nginx
  releaseName: ingress-nginx
  interval: 15m
  chart:
    spec:
      chart: ingress-nginx
      sourceRef:
        kind: HelmRepository
        name: ingress-nginx
        namespace: ingress-nginx
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    global:
      image:
        registry: registry.k8s.io
    serviceAccount:
      create: true
      automountServiceAccountToken: false
    fullnameOverride: ingress-nginx
    controller:
      extraVolumes:
        - name: kube-api-access-cm
          projected:
            defaultMode: 420
            sources:
              - serviceAccountToken:
                  expirationSeconds: 3607
                  path: token
              - configMap:
                  items:
                    - key: ca.crt
                      path: ca.crt
                  name: kube-root-ca.crt
              - downwardAPI:
                  items:
                    - fieldRef:
                        apiVersion: v1
                        fieldPath: metadata.namespace
                      path: namespace
      extraVolumeMounts:
        - name: kube-api-access-cm
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          readOnly: true
      image:
        image: ingress-nginx/controller
        tag: "v1.12.0"
        digest: sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa
        readOnlyRootFilesystem: false
      admissionWebhooks:
        patch:
          image:
            image: ingress-nginx/kube-webhook-certgen
            tag: v1.5.0
            digest: sha256:aaafd456bda110628b2d4ca6296f38731a3aaf0bf7581efae824a41c770a8fc4
      ingressClassResource:
        controllerValue: k8s.io/ingress-nginx
        default: true
        enabled: true
        name: nginx
      config:
        client-body-buffer-size: 100M
        client-body-timeout: 120
        client-header-timeout: 120
        enable-brotli: "true"
        enable-real-ip: "true"
        hsts-max-age: 31449600
        keep-alive-requests: 10000
        keep-alive: 120
        log-format-escape-json: "true"
        log-format-upstream: >
          {"time": "$time_iso8601", "remote_addr": "$proxy_protocol_addr", "x_forwarded_for": "$proxy_add_x_forwarded_for",
          "request_id": "$req_id", "remote_user": "$remote_user", "bytes_sent": $bytes_sent, "request_time": $request_time,
          "status": $status, "vhost": "$host", "request_proto": "$server_protocol", "path": "$uri", "request_query": "$args",
          "request_length": $request_length, "duration": $request_time, "method": "$request_method", "http_referrer": "$http_referer",
          "http_user_agent": "$http_user_agent"}
        proxy-body-size: 0
        proxy-buffer-size: 16k
        ssl-protocols: TLSv1.3 TLSv1.2
      minAvailable: 1
      service:
        externalTrafficPolicy: Local
        annotations:
          service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: '/healthz'
          # nginx.ingress.kubernetes.io/force-ssl-redirect: "true" - once cert-manager is configured
      autoscaling:
        enabled: true
        minReplicas: 3
        maxReplicas: 5
        targetCPUUtilizationPercentage: 90
        targetMemoryUtilizationPercentage: 90
      podSecurityContext:
        runAsNonRoot: true
        runAsUser: 65532
        runAsGroup: 65532
        seccompProfile:
          type: 'RuntimeDefault'
        readOnlyRootFilesystem: true
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: ingress-nginx
              app.kubernetes.io/instance: ingress-nginx
              app.kubernetes.io/component: controller
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
        limits:
          memory: 900Mi
endsnippet


snippet flux-helmrelease "Flux HelmRelease CRD (Install velero)" !b
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: flux-apps
  namespace: flux-system
spec:
  interval: 1m0s
  ref:
    branch: main
  url: https://github.com/akhan4u/flux-apps
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: flux-apps
  namespace: flux-system
spec:
  interval: 30m0s
  path: ./kustomize
  prune: true
  retryInterval: 2m0s
  sourceRef:
    kind: GitRepository
    name: flux-apps
  timeout: 3m0s
  wait: true
  postBuild:
    substituteFrom:
      - kind: ConfigMap
        name: cluster-config-main
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: velero
  namespace: velero
spec:
  interval: 15m
  timeout: 15m
  chart:
    spec:
      chart: velero
      sourceRef:
        kind: HelmRepository
        name: velero
        namespace: velero
      interval: 5m
  releaseName: velero
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  # Default values: https://github.com/vmware-tanzu/helm-charts/blob/velero-8.1.0/charts/velero/values.yaml
  values:
    metrics:
      serviceMonitor:
        enabled: true
    configuration:
      backupStorageLocation:
        - bucket: ${velero_backup_s3_bucket}
          provider: aws
      volumeSnapshotLocation:
        - config:
            region: ${velero_snapshot_aws_region}
          provider: aws
    initContainers:
      - name: velero-plugin-for-aws
        image: velero/velero-plugin-for-aws:v1.11.0
        volumeMounts:
          - mountPath: /target
            name: plugins
    credentials:
      useSecret: true
      secretContents:
        cloud: |
          [default]
          aws_access_key_id=${velero_aws_access_key_id}
          aws_secret_access_key=${velero_aws_secret_access_key}
endsnippet

snippet flux-helmrepo "Flux HelmRepo CRD (Install prometheus-operator-crds)" !b
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: prometheus-community
  namespace: flux-system
spec:
  # Interval between automatic updates of the repository (24 hours)
  interval: 24h
  # URL of the Helm repository
  url: https://prometheus-community.github.io/helm-charts
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: prometheus-operator-crds
  namespace: flux-system
spec:
  # Interval between automatic updates (60 minutes)
  interval: 60m
  chart:
    spec:
      # Name of the Helm chart to use
      chart: prometheus-operator-crds
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
      # Interval between automatic updates of the chart (5 minutes)
      interval: 5m
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  # List of resources to be included in the Kustomization
  - helm-repo.yaml
  - helm-release.yaml
endsnippet

snippet flux-kustomization "Flux Kustomization CRD (Install cert-manager)" !b
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  # Use a specific version instead 'latest' if you want to install a specific version
  - https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.crds.yaml
endsnippet

snippet flux-kustomization "Flux Kustomization CRD (Install external-secrets)" !b
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  # Use a specific version instead 'main' if you want to install a specific version
  - https://raw.githubusercontent.com/external-secrets/external-secrets/main/deploy/crds/bundle.yaml
endsnippet

snippet prom-servicemonitor "Prometheus Service Monitor resource" !b
# The Prometheus Operator includes a Custom Resource Definition that allows the definition of
# the ServiceMonitor. The ServiceMonitor is used to define an application you wish to scrape metrics
# from within Kubernetes, the controller will action the ServiceMonitors we define and automatically
# build the required Prometheus configuration. Within the ServiceMonitor we specify the Kubernetes
# Labels that the Operator can use to identify the Kubernetes Service which in turn then identifies the Pods, that we wish to monitor.

# This Kubernetes Resource uses the monitoring.coreos.com/v1 API Version that was installed into
# Kubernetes by the Prometheus Operator, as explained previously. It uses the namespaceSelector
# to specify the Kubernetes Namespace in which we wish to locate the Service, in this example
# above we are selecting within the prometheus namespace. It then uses the selector to specify
# that it must match the Label operated-prometheus being set as “true”.

# Under the endpoints key we must specify one or more scrape targets for the target service.
# In this example it will scrape each Pod it selects on TCP port 9090 on the URL /metrics every 30 seconds.
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    serviceMonitorSelector: prometheus
  name: prometheus
  namespace: prometheus
spec:
  endpoints:
  - interval: 30s
    targetPort: 9090
    path: /metrics
  namespaceSelector:
    matchNames:
    - prometheus
  selector:
    matchLabels:
      operated-prometheus: "true"
endsnippet


snippet job "Kubernetes Job to check Disk IOPS with FIO" !b
# Create PVC
# kubectl apply -f pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: fio-pvc
spec:
  storageClassName: STORAGE_CLASS_NAME
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 250Gi

# Create the Benchmark Job
# kubectl apply -f fio-job.yaml
---
apiVersion: batch/v1
kind: Job
metadata:
  name: fio-benchmark
spec:
  backoffLimit: 2
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: fio-benchmark
          image: ubuntu:22.04
          command:
            - bash
            - -c
            - |
              apt update
              apt install -y fio
              echo "=== Storage Performance Test ==="
              date
              fio --name=benchmark_test \
                  --filename=/mnt/data/testfile \
                  --rw=write \
                  --bs=1M \
                  --ioengine=libaio \
                  --iodepth=64 \
                  --direct=1 \
                  --size=200G \
                  --numjobs=8 \
                  --runtime=300 \
                  --time_based \
                  --group_reporting
          volumeMounts:
            - mountPath: /mnt/data
              name: fio-mount
      volumes:
        - name: fio-mount
          persistentVolumeClaim:
            claimName: fio-pvc

# Understanding the FIO Parameters
#   • --rw=write: Test write performance
#   • --bs=1M: Use 1MB block size for large sequential writes
#   • --ioengine=libaio: Use Linux async I/O for better performance
#   • --iodepth=64: Queue depth for async I/O
#   • --direct=1: Use direct I/O, bypassing the OS cache
#   • --size=200G: Total size of the test file
#   • --numjobs=8: Number of parallel processes
#   • --runtime=300: Run test for 5 minutes
#   • --time_based: Run for the specified time rather than until size is reached
#   • --group_reporting: Aggregate results from all jobs

# Interpreting Results
# The output will include:
#   • Write bandwidth (MB/s)
#   • IOPS (Input/Output Operations Per Second)
#   • Latency statistics
#   • CPU utilization
#   • I/O depth distribution

# Key metrics to compare:
#   • Higher bandwidth indicates better throughput
#   • Lower latency suggests better response times
#   • Higher IOPS shows better small I/O performance
endsnippet

snippet playbook "Ansible playbook for roles execution" !b
# Set or export ANSIBLE_ROLES_PATH=/path/to/roles/dir
---
  - hosts: all # or 'localhost'
    become: yes
    roles:
      - host-common
endsnippet

snippet playbook "Ansible playbook with Sample tasks" !b
---
  - hosts: localhost
    gather_facts: yes
    tasks:
      - debug:
          msg: '{{ansible_distribution}}'
        tags:
          - ansible_facts
      - debug:
          msg: '{{ansible_machine}}'
        tags:
          - ansible_facts
      - debug:
          msg: '{{ansible_system}}'
        tags:
          - ansible_facts
      - debug:
          msg: '{{ansible_date_time.time}}'
        tags:
          - ansible_facts
endsnippet

snippet mani "Add mani spec" !b
specs:
  custom:
    output: table
    parallel: true

targets:
  all:
    all: true

themes:
  custom:
    table:
      border:
        around: true
        columns: true
        header: true
        rows: true

tasks:
  git-status:
    desc: show working tree status
    spec: custom
    target: all
    cmd: git status

  pull:
    desc: pull remote changes on local
    spec: custom
    cmd: git fetch --all && git pull --all

  git-last-commit-msg:
    desc: show last commit
    cmd: |
      git log -1 --pretty="%h %ad | %an <%ae>" -s --date=short
      git log -1 --format='%B'

  git-last-commit-date:
    desc: show last commit date
    cmd: |
      git log -1 --format="%cd (%cr)" -n 1 --date=format:"%d  %b %y" \
      | sed 's/ //'
    target: all

  git-branch:
    desc: show current git branch
    cmd: git rev-parse --abbrev-ref HEAD

  search-mr:
    desc: Searches Gitlab MRs In A Repo with specified ticket number
    spec: custom
    cmd: glab mr list -A --search $SEARCH

  list-pr:
    desc: List Recent 5 GitHub PRs In A Repo
    spec: custom
    cmd: gh pr list --limit 5
    target: all

  list-mr:
    desc: List Recent 5 Gitlab MRs In A Repo (Not Draft)
    spec: custom
    cmd: glab mr list --not-draft --per-page 5

  list-mrd:
    desc: List Recent 5 Gitlab MRs In A Repo (Draft)
    spec: custom
    cmd: glab mr list --draft --per-page 5

  git-overview:
    desc: show branch, local and remote diffs, last commit and date
    spec: custom
    target: all
    theme: custom
    commands:
      - task: git-branch
      - task: git-last-commit-msg
      - task: git-last-commit-date
endsnippet
